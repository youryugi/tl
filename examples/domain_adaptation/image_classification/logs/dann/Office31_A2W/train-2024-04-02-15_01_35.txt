Namespace(arch='resnet50', batch_size=32, bottleneck_dim=256, data='OfficeCaltech', epochs=20, iters_per_epoch=1000, log='logs/dann/Office31_A2W', lr=0.01, lr_decay=0.75, lr_gamma=0.001, momentum=0.9, no_hflip=False, no_pool=False, norm_mean=(0.485, 0.456, 0.406), norm_std=(0.229, 0.224, 0.225), per_class_eval=False, phase='train', print_freq=100, ratio=[0.75, 1.3333333333333333], resize_size=224, root='data/OfficeCaltech', scale=[0.08, 1.0], scratch=False, seed=1, source=['A'], target=['W'], trade_off=1.0, train_resizing='default', val_resizing='default', weight_decay=0.001, workers=2)
dann.py:40: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.
  warnings.warn('You have chosen to seed training. '
train_transform:  Compose(
    Compose(
    ResizeImage(size=(256, 256))
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)
)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
)
val_transform:  Compose(
    Compose(
    ResizeImage(size=(256, 256))
    CenterCrop(size=(224, 224))
)
    ToTensor()
    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
)
=> using model 'resnet50'
lr: 0.001
Traceback (most recent call last):
  File "dann.py", line 266, in <module>
    main(args)
  File "dann.py", line 116, in main
    train(train_source_iter, train_target_iter, classifier, domain_adv, optimizer,
  File "dann.py", line 174, in train
    transfer_loss = domain_adv(f_s, f_t)
  File "C:\Users\79152\.conda\envs\d38\lib\site-packages\torch\nn\modules\module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "C:\Users\79152\Desktop\Transfer-Learning-Library-master\examples\domain_adaptation\image_classification\../../..\tllib\alignment\dann.py", line 72, in forward
    f = self.grl(torch.cat((f_s, f_t), dim=0))
  File "C:\Users\79152\.conda\envs\d38\lib\site-packages\torch\nn\modules\module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "C:\Users\79152\Desktop\Transfer-Learning-Library-master\examples\domain_adaptation\image_classification\../../..\tllib\modules\grl.py", line 71, in forward
    coeff = np.float(
  File "C:\Users\79152\.conda\envs\d38\lib\site-packages\numpy\__init__.py", line 305, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
